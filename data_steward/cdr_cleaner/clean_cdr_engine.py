# Python imports
import inspect
import logging
from concurrent.futures import TimeoutError as TOError

# Third party imports
import google.cloud.bigquery as gbq
from google.cloud.exceptions import GoogleCloudError
import googleapiclient
import oauth2client

# Project imports
import app_identity
import bq_utils
from utils import bq
import sandbox
from cdr_cleaner.cleaning_rules.base_cleaning_rule import BaseCleaningRule
from cdr_cleaner.clean_cdr import DATA_STAGE_RULES_MAPPING
from constants import bq_utils as bq_consts
from constants.cdr_cleaner import clean_cdr as cdr_consts
from constants.cdr_cleaner import clean_cdr_engine as ce_consts

LOGGER = logging.getLogger(__name__)


def add_console_logging(add_handler):
    """

    This config should be done in a separate module, but that can wait
    until later.  Useful for debugging.

    """
    logging.basicConfig(
        level=logging.INFO,
        filename=ce_consts.FILENAME,
        filemode='a',
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    if add_handler:
        handler = logging.StreamHandler()
        handler.setLevel(logging.INFO)
        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(name)s - %(message)s')
        handler.setFormatter(formatter)
        logging.getLogger('').addHandler(handler)


def clean_dataset(project=None,
                  statements=None,
                  data_stage=cdr_consts.DataStage.UNSPECIFIED):
    """
       Run the assigned cleaning rules.

       :param project:  the project name
       :param statements:  a list of dictionary objects to run the query
       :param data_stage:  an enum to indicate what stage of the cleaning this is
       """
    if project is None or project == '' or project.isspace():
        project = app_identity.get_application_id()
        LOGGER.info('Project name not provided.  Using default.')

    if statements is None:
        statements = []

    failures = 0
    successes = 0

    statement_length = len(statements)
    for index, statement in enumerate(statements):
        rule_query = statement.get(cdr_consts.QUERY, '')
        legacy_sql = statement.get(cdr_consts.LEGACY_SQL, False)
        destination_table = statement.get(cdr_consts.DESTINATION_TABLE, None)
        retry = statement.get(cdr_consts.RETRY_COUNT,
                              bq_consts.BQ_DEFAULT_RETRY_COUNT)
        disposition = statement.get(cdr_consts.DISPOSITION,
                                    bq_consts.WRITE_EMPTY)
        destination_dataset = statement.get(cdr_consts.DESTINATION_DATASET,
                                            None)
        batch = statement.get(cdr_consts.BATCH, None)

        module_name = statement.get(cdr_consts.MODULE_NAME,
                                    cdr_consts.MODULE_NAME_DEFAULT_VALUE)
        function_name = statement.get(cdr_consts.FUNCTION_NAME,
                                      cdr_consts.FUNCTION_NAME_DEFAULT_VALUE)

        try:
            LOGGER.info(
                f"Running query {index} out of {statement_length} generated by "
                f"{module_name}.{function_name} \n{rule_query}")

            results = bq_utils.query(rule_query,
                                     use_legacy_sql=legacy_sql,
                                     destination_table_id=destination_table,
                                     retry_count=retry,
                                     write_disposition=disposition,
                                     destination_dataset_id=destination_dataset,
                                     batch=batch)

        except (oauth2client.client.HttpAccessTokenRefreshError,
                googleapiclient.errors.HttpError) as exp:

            LOGGER.exception(
                format_failure_message(project_id=project,
                                       statement=statement,
                                       exception=exp))
            failures += 1
            continue

        # wait for job to finish
        query_job_id = results['jobReference']['jobId']
        incomplete_jobs = bq_utils.wait_on_jobs([query_job_id])
        if incomplete_jobs:
            failures += 1
            raise bq_utils.BigQueryJobWaitError(incomplete_jobs)

        # check if the job is complete and an error has occurred
        is_errored, error_message = bq_utils.job_status_errored(query_job_id)

        if is_errored:
            LOGGER.error(
                format_failure_message(project_id=project,
                                       statement=statement,
                                       exception=error_message))
            failures += 1
        else:
            if destination_table is not None:
                updated_rows = results.get("totalRows")
                if updated_rows is not None:
                    LOGGER.info(
                        f"Query returned {updated_rows} rows for {destination_dataset}.{destination_table}"
                    )

            successes += 1

    if successes > 0:
        LOGGER.info(
            f"Successfully applied {successes} clean rules for {project}.{data_stage}"
        )
    else:
        LOGGER.warning(
            f"No clean rules successfully applied to {project}.{data_stage}")

    if failures > 0:
        LOGGER.warning(
            f"Failed to apply {failures} clean rules for {project}.{data_stage}"
        )
    else:
        LOGGER.info(
            f"There were no failures in applying cleaning rules for {project}.{data_stage}"
        )


def run_queries(client, dataset_id, query_list, module_info_dict):
    """
    Runs queries from the list of query_dicts

    :param client: BigQuery client
    :param dataset_id: identifies the dataset to clean
    :param query_list: list of query_dicts generated by a cleaning rule
    :param module_info_dict: contains information about the query function
    :return: integers indicating the number of queries that succeeded and failed
    """
    query_count = len(query_list)
    successes, failures = 0, 0
    for index, query_dict in enumerate(query_list):
        try:
            LOGGER.info(
                ce_consts.QUERY_RUN_MESSAGE_TEMPLATE.render(
                    index, query_count, **module_info_dict, **query_dict))
            job_config = gbq.job.QueryJobConfig()
            if query_dict.get(cdr_consts.DESTINATION_TABLE) is not None:
                destination_dataset_id = query_dict.get(
                    cdr_consts.DESTINATION_DATASET)
                destination_dataset_ref = gbq.DatasetReference(
                    client.project, destination_dataset_id)
                destination_table_id = query_dict.get(
                    cdr_consts.DESTINATION_TABLE)
                destination_table_ref = gbq.table.TableReference(
                    destination_dataset_ref, destination_table_id)

                job_config.destination = destination_table_ref
                job_config.use_legacy_sql = query_dict.get(
                    cdr_consts.LEGACY_SQL, False)
                # allow_large_results can only be used if use_legacy_sql=True
                job_config.allow_large_results = job_config.use_legacy_sql
                job_config.write_disposition = query_dict.get(
                    cdr_consts.DISPOSITION, bq_consts.WRITE_EMPTY)

            query_job = client.query(query=query_dict.get(cdr_consts.QUERY),
                                     job_config=job_config,
                                     job_id_prefix=f'clean_{dataset_id}_')

            # wait for job to complete
            query_job.result()
        except (GoogleCloudError, TOError) as exp:
            LOGGER.exception(
                ce_consts.FAILURE_MESSAGE_TEMPLATE.render(client.project,
                                                          **module_info_dict,
                                                          **query_dict,
                                                          exception=exp))
            failures += 1
            continue
        else:
            successes += 1
    return successes, failures


def get_module_info(query_function):
    """
    Extracts module information about the cleaning rule

    :param query_function: a function that generates a list of query dictionaries
    :return: dictionary containing the module information
        keys: module_name, function_name, line_no
    """
    function_name = query_function.__name__
    module_name = inspect.getmodule(query_function).__name__
    _, line_no = inspect.getsourcelines(query_function)
    module_info_dict = {
        cdr_consts.MODULE_NAME: module_name,
        cdr_consts.FUNCTION_NAME: function_name,
        cdr_consts.LINE_NO: line_no
    }
    return module_info_dict


def clean_dataset_v1(project_id=None,
                     dataset_id=None,
                     rules=None,
                     data_stage=cdr_consts.DataStage.UNSPECIFIED):
    """
    Run the assigned cleaning rules.

    :param project_id: identifies the project
    :param dataset_id: identifies the dataset to clean
    :param rules: a list of cleaning rule objects/functions as tuples
    :param data_stage: an enum to indicate what stage of the cleaning this is
    """
    if project_id is None or project_id == '' or project_id.isspace():
        project_id = app_identity.get_application_id()
        LOGGER.info(f"project_id not provided, using default {project_id}")

    if rules is None:
        rules = DATA_STAGE_RULES_MAPPING.get(data_stage, [])

    # Set up
    client = bq.get_client(project_id=project_id)
    sandbox_dataset_id = sandbox.create_sandbox_dataset(project_id=project_id,
                                                        dataset_id=dataset_id)
    successful_rules, failed_rules = 0, 0
    for rule in rules:
        clazz = rule[0]
        try:
            instance = clazz(project_id, dataset_id, sandbox_dataset_id)
        except TypeError:
            # raised when called with the 3 parameters and only 2 are needed
            query_list = clazz(project_id, dataset_id)
            query_function = clazz
        else:
            # should eventually be the main component of this function.
            # Everything should transition to using a common base class.
            if isinstance(instance, BaseCleaningRule):

                keywords = rule[-1]
                if isinstance(keywords, dict):
                    positionals = rule[1:-1]
                else:
                    keywords = {}
                    positionals = rule[1:]

                query_function = instance.get_query_specs

                # setup
                try:
                    instance.setup_rule(client)
                except NotImplementedError:
                    logging.warning(
                        f"setup rule not implemented for {query_function.__name__}"
                    )

                query_list = instance.get_query_specs()
            else:
                query_list = clazz(project_id, dataset_id, sandbox_dataset_id)
                query_function = clazz

        module_info_dict = get_module_info(query_function)
        successes, failures = run_queries(client, dataset_id, query_list,
                                          module_info_dict)
        LOGGER.info(
            f"Status: {successes} successes and {failures} failures out of "
            f"{len(query_list)} queries for clean rule {module_info_dict}")
        if failures > 0:
            failed_rules += 1
        if successes > 0:
            successful_rules += 1

    if successful_rules > 0:
        LOGGER.info(
            f"Successfully applied {successful_rules} clean rules for {project_id}.{data_stage}"
        )
    else:
        LOGGER.warning(
            f"No clean rules successfully applied to {project_id}.{data_stage}")

    if failed_rules > 0:
        LOGGER.warning(
            f"Failed to apply {failed_rules} clean rules for {project_id}.{data_stage}"
        )
    else:
        LOGGER.info(
            f"There were no failures in applying cleaning rules for {project_id}.{data_stage}"
        )


def format_failure_message(project_id, statement, exception):

    query = statement.get(cdr_consts.QUERY, '')
    destination_table = statement.get(cdr_consts.DESTINATION_TABLE, None)
    disposition = statement.get(cdr_consts.DISPOSITION, bq_consts.WRITE_EMPTY)
    destination_dataset_id = statement.get(cdr_consts.DESTINATION_DATASET, None)
    module_name = statement.get(cdr_consts.MODULE_NAME,
                                cdr_consts.MODULE_NAME_DEFAULT_VALUE)
    function_name = statement.get(cdr_consts.FUNCTION_NAME,
                                  cdr_consts.FUNCTION_NAME_DEFAULT_VALUE)
    line_no = statement.get(cdr_consts.LINE_NO,
                            cdr_consts.LINE_NO_DEFAULT_VALUE)

    return ce_consts.FAILURE_MESSAGE_TEMPLATE.format(
        module_name=module_name,
        function_name=function_name,
        line_no=line_no,
        project_id=project_id,
        destination_dataset_id=destination_dataset_id,
        destination_table=destination_table,
        disposition=disposition,
        query=query,
        exception=exception)
