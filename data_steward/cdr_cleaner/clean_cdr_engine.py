# Python imports
import inspect
import logging
from concurrent.futures import TimeoutError as TOError

# Third party imports
import google.cloud.bigquery as gbq
from google.cloud.exceptions import GoogleCloudError
import googleapiclient
import oauth2client

# Project imports
import app_identity
import bq_utils
from utils import bq
import sandbox
from cdr_cleaner.cleaning_rules.base_cleaning_rule import BaseCleaningRule
from constants import bq_utils as bq_consts
from constants.cdr_cleaner import clean_cdr as cdr_consts
from constants.cdr_cleaner import clean_cdr_engine as ce_consts

LOGGER = logging.getLogger(__name__)


def add_console_logging(add_handler):
    """

    This config should be done in a separate module, but that can wait
    until later.  Useful for debugging.

    """
    logging.basicConfig(
        level=logging.INFO,
        filename=ce_consts.FILENAME,
        filemode='a',
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    if add_handler:
        handler = logging.StreamHandler()
        handler.setLevel(logging.INFO)
        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(name)s - %(message)s')
        handler.setFormatter(formatter)
        logging.getLogger('').addHandler(handler)


def clean_dataset(project=None,
                  statements=None,
                  data_stage=cdr_consts.DataStage.UNSPECIFIED):
    """
       Run the assigned cleaning rules.

       :param project:  the project name
       :param statements:  a list of dictionary objects to run the query
       :param data_stage:  an enum to indicate what stage of the cleaning this is
       """
    if project is None or project == '' or project.isspace():
        project = app_identity.get_application_id()
        LOGGER.info('Project name not provided.  Using default.')

    if statements is None:
        statements = []

    failures = 0
    successes = 0

    statement_length = len(statements)
    for index, statement in enumerate(statements):
        rule_query = statement.get(cdr_consts.QUERY, '')
        legacy_sql = statement.get(cdr_consts.LEGACY_SQL, False)
        destination_table = statement.get(cdr_consts.DESTINATION_TABLE, None)
        retry = statement.get(cdr_consts.RETRY_COUNT,
                              bq_consts.BQ_DEFAULT_RETRY_COUNT)
        disposition = statement.get(cdr_consts.DISPOSITION,
                                    bq_consts.WRITE_EMPTY)
        destination_dataset = statement.get(cdr_consts.DESTINATION_DATASET,
                                            None)
        batch = statement.get(cdr_consts.BATCH, None)

        module_name = statement.get(cdr_consts.MODULE_NAME,
                                    cdr_consts.MODULE_NAME_DEFAULT_VALUE)
        function_name = statement.get(cdr_consts.FUNCTION_NAME,
                                      cdr_consts.FUNCTION_NAME_DEFAULT_VALUE)

        try:
            LOGGER.info(
                f"Running query {index} out of {statement_length} generated by "
                f"{module_name}.{function_name} \n{rule_query}")

            results = bq_utils.query(rule_query,
                                     use_legacy_sql=legacy_sql,
                                     destination_table_id=destination_table,
                                     retry_count=retry,
                                     write_disposition=disposition,
                                     destination_dataset_id=destination_dataset,
                                     batch=batch)

        except (oauth2client.client.HttpAccessTokenRefreshError,
                googleapiclient.errors.HttpError) as exp:

            LOGGER.exception(
                format_failure_message(project_id=project,
                                       statement=statement,
                                       exception=exp))
            failures += 1
            continue

        # wait for job to finish
        query_job_id = results['jobReference']['jobId']
        incomplete_jobs = bq_utils.wait_on_jobs([query_job_id])
        if incomplete_jobs:
            failures += 1
            raise bq_utils.BigQueryJobWaitError(incomplete_jobs)

        # check if the job is complete and an error has occurred
        is_errored, error_message = bq_utils.job_status_errored(query_job_id)

        if is_errored:
            LOGGER.error(
                format_failure_message(project_id=project,
                                       statement=statement,
                                       exception=error_message))
            failures += 1
        else:
            if destination_table is not None:
                updated_rows = results.get("totalRows")
                if updated_rows is not None:
                    LOGGER.info(
                        f"Query returned {updated_rows} rows for {destination_dataset}.{destination_table}"
                    )

            successes += 1

    if successes > 0:
        LOGGER.info(
            f"Successfully applied {successes} clean rules for {project}.{data_stage}"
        )
    else:
        LOGGER.warning(
            f"No clean rules successfully applied to {project}.{data_stage}")

    if failures > 0:
        LOGGER.warning(
            f"Failed to apply {failures} clean rules for {project}.{data_stage}"
        )
    else:
        LOGGER.info(
            f"There were no failures in applying cleaning rules for {project}.{data_stage}"
        )


def get_destination_ref(project_id, query_dict):
    """
    Fetch BQ table reference to destination table

    :param project_id: Identifies the project
    :param query_dict: dictionary for the query
    :return: BQ TableReference object to destination
    """
    destination_dataset_id = query_dict.get(cdr_consts.DESTINATION_DATASET)
    destination_dataset_ref = gbq.DatasetReference(project_id,
                                                   destination_dataset_id)
    destination_table_id = query_dict.get(cdr_consts.DESTINATION_TABLE)
    destination_table_ref = gbq.table.TableReference(destination_dataset_ref,
                                                     destination_table_id)
    return destination_table_ref


def generate_job_config(project_id, query_dict):
    """
    Generates BigQuery job_configuration object

    :param project_id: Identifies the project
    :param query_dict: dictionary for the query
    :return: BQ job_configuration object
    """
    job_config = gbq.job.QueryJobConfig()
    if query_dict.get(cdr_consts.DESTINATION_TABLE) is None:
        return job_config

    destination_table = get_destination_ref(project_id, query_dict)

    job_config.destination = destination_table
    job_config.use_legacy_sql = query_dict.get(cdr_consts.LEGACY_SQL, False)
    # allow_large_results can only be used if use_legacy_sql=True
    job_config.allow_large_results = job_config.use_legacy_sql
    job_config.write_disposition = query_dict.get(cdr_consts.DISPOSITION,
                                                  bq_consts.WRITE_EMPTY)
    return job_config


def run_queries(client, query_list, rule_info):
    """
    Runs queries from the list of query_dicts

    :param client: BigQuery client
    :param query_list: list of query_dicts generated by a cleaning rule
    :param rule_info: contains information about the query function
    :return: integers indicating the number of queries that succeeded and failed
    """
    query_count = len(query_list)
    successes, failures = [], []
    for index, query_dict in enumerate(query_list):
        try:
            LOGGER.info(
                ce_consts.QUERY_RUN_MESSAGE_TEMPLATE.render(
                    index, query_count, **rule_info, **query_dict))
            job_config = generate_job_config(client.project, query_dict)

            module_name = rule_info[cdr_consts.MODULE_NAME]
            query_job = client.query(query=query_dict.get(cdr_consts.QUERY),
                                     job_config=job_config,
                                     job_id_prefix=f'clean_{module_name}_')

            # wait for job to complete
            query_job.result()
        except (GoogleCloudError, TOError) as exp:
            LOGGER.exception(
                ce_consts.FAILURE_MESSAGE_TEMPLATE.render(client.project,
                                                          **rule_info,
                                                          **query_dict,
                                                          exception=exp))
            failures.append(query_dict[cdr_consts.QUERY])
            continue
        else:
            successes.append(query_dict[cdr_consts.QUERY])
    return successes, failures


def infer_rule(clazz, project_id, dataset_id, sandbox_dataset_id):
    """
    Extract information about the cleaning rule

    :param clazz: Clean rule class or old style clean function
    :param project_id: identifies the project
    :param dataset_id: identifies the dataset to clean
    :param sandbox_dataset_id: identifies the dataset to store sandbox tables
    :return: rule_info: dictionary of information about the rule
        keys:
            query_function: function that generates query_list
            setup_function: function that sets up the tables for the rule
            function_name: name of the query function
            module_name: name of the module containing the function
            line_no: points to the source line where query_function is
    """
    if inspect.isclass(clazz):
        instance = clazz(project_id, dataset_id, sandbox_dataset_id)

        query_function = instance.get_query_specs
        setup_function = instance.setup_rule
        function_name = query_function.__name__
        module_name = inspect.getmodule(query_function).__name__
        line_no = inspect.getsourcelines(query_function)[1]
    else:
        function_name = clazz.__name__
        module_name = inspect.getmodule(clazz).__name__
        line_no = inspect.getsourcelines(clazz)[1]

        def query_function():
            """
            Imitates base class get_query_specs()

            :return: list of query dicts generated by rule
            """
            if len(inspect.signature(clazz).parameters) == 3:
                return clazz(project_id, dataset_id, sandbox_dataset_id)
            return clazz(project_id, dataset_id)

        def setup_function(client):
            """
            Imitates base class setup_rule()
            """
            pass

    rule_info = {
        cdr_consts.QUERY_FUNCTION: query_function,
        cdr_consts.SETUP_FUNCTION: setup_function,
        cdr_consts.FUNCTION_NAME: function_name,
        cdr_consts.MODULE_NAME: module_name,
        cdr_consts.LINE_NO: line_no,
    }
    return query_function, setup_function, rule_info


def get_query_list(project_id=None,
                   dataset_id=None,
                   rules=None,
                   data_stage=cdr_consts.DataStage.UNSPECIFIED):
    """
    Generates list of all query_dicts that will be run on the dataset

    :param project_id: identifies the project
    :param dataset_id: identifies the dataset to clean
    :param rules: a list of cleaning rule objects/functions as tuples
    :param data_stage: an enum to indicate what stage of the cleaning this is
    :return list of all query_dicts that will be run on the dataset
    """
    if project_id is None or project_id == '' or project_id.isspace():
        project_id = app_identity.get_application_id()
        LOGGER.info(f"project_id not provided, using default {project_id}")

    sandbox_dataset_id = sandbox.get_sandbox_dataset_id(dataset_id)

    all_queries_list = []
    for rule in rules:
        clazz = rule[0]
        query_function, _, rule_info = infer_rule(clazz, project_id, dataset_id,
                                                  sandbox_dataset_id)
        # added to test closure
        project_id = 'incorrect_proj'
        dataset_id = 'incorrect_ds'
        query_list = query_function()
        all_queries_list.extend(query_list)
    return all_queries_list


def clean_dataset_v1(project_id=None,
                     dataset_id=None,
                     rules=None,
                     data_stage=cdr_consts.DataStage.UNSPECIFIED):
    """
    Run the assigned cleaning rules.

    :param project_id: identifies the project
    :param dataset_id: identifies the dataset to clean
    :param rules: a list of cleaning rule objects/functions as tuples
    :param data_stage: an enum to indicate what stage of the cleaning this is
    """
    if project_id is None or project_id == '' or project_id.isspace():
        project_id = app_identity.get_application_id()
        LOGGER.info(f"project_id not provided, using default {project_id}")

    # Set up
    client = bq.get_client(project_id=project_id)
    sandbox_dataset_id = sandbox.create_sandbox_dataset(project_id=project_id,
                                                        dataset_id=dataset_id)
    successful_rules, failed_rules = {}, {}
    for rule in rules:
        clazz = rule[0]
        query_function, setup_function, rule_info = infer_rule(
            clazz, project_id, dataset_id, sandbox_dataset_id)
        setup_function(client)
        query_list = query_function()
        successes, failures = run_queries(client, query_list, rule_info)
        LOGGER.info(
            f"Status: {len(successes)} successes and {len(failures)} failures out of "
            f"{len(query_list)} queries for clean rule {rule_info[cdr_consts.MODULE_NAME]}"
        )
        if failures:
            failed_rules[rule_info[cdr_consts.MODULE_NAME]] = failures
        if successes:
            successful_rules[rule_info[cdr_consts.MODULE_NAME]] = successes

    if len(successful_rules) > 0:
        LOGGER.info(
            f"Successfully applied {len(successful_rules)} clean rules for {project_id}.{data_stage}"
        )
    else:
        LOGGER.warning(
            f"No clean rules successfully applied to {project_id}.{data_stage}")

    if len(failed_rules) > 0:
        LOGGER.warning(
            f"Failed to apply {len(failed_rules)} clean rules for {project_id}.{data_stage}"
        )
    else:
        LOGGER.info(
            f"There were no failures in applying cleaning rules for {project_id}.{data_stage}"
        )


def format_failure_message(project_id, statement, exception):

    query = statement.get(cdr_consts.QUERY, '')
    destination_table = statement.get(cdr_consts.DESTINATION_TABLE, None)
    disposition = statement.get(cdr_consts.DISPOSITION, bq_consts.WRITE_EMPTY)
    destination_dataset_id = statement.get(cdr_consts.DESTINATION_DATASET, None)
    module_name = statement.get(cdr_consts.MODULE_NAME,
                                cdr_consts.MODULE_NAME_DEFAULT_VALUE)
    function_name = statement.get(cdr_consts.FUNCTION_NAME,
                                  cdr_consts.FUNCTION_NAME_DEFAULT_VALUE)
    line_no = statement.get(cdr_consts.LINE_NO,
                            cdr_consts.LINE_NO_DEFAULT_VALUE)

    return ce_consts.FAILURE_MESSAGE_TEMPLATE.render(
        module_name=module_name,
        function_name=function_name,
        line_no=line_no,
        project_id=project_id,
        destination_dataset_id=destination_dataset_id,
        destination_table=destination_table,
        disposition=disposition,
        query=query,
        exception=exception)
