{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "1. matplotlib MUST be in 3.1.0; 3.1.1 ruins the heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Across-Site Statistics for Duplicates\n",
    "\n",
    "### NOTE: Aggregate info is weighted by the contribution of each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets = []\n",
    "\n",
    "fn1 = 'duplicates_table_sheets_data_analytics.xlsx'\n",
    "file_names = [fn1]\n",
    "\n",
    "s1 = 'condition_occurrence'\n",
    "s2 = 'drug_exposure'\n",
    "s3 = 'measurement'\n",
    "s4 = 'observation'\n",
    "s5 = 'procedure_occurrence'\n",
    "s6 = 'visit_occurrence'\n",
    "\n",
    "sheet_names = [s1, s2, s3, s4, s5, s6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_sheets = []\n",
    "\n",
    "for file in file_names:\n",
    "    for sheet in sheet_names:\n",
    "        s = pd.read_excel(file, sheet)\n",
    "        table_sheets.append(s)\n",
    "\n",
    "hpo_id_cols = table_sheets[0]['hpo_ids']\n",
    "date_cols = table_sheets[0].columns[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing typos in the sheets; in some of the earlire reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, table_id in enumerate(sheet_names):\n",
    "    under_encountered = False\n",
    "    start_idx, end_idx = 0, 0\n",
    "    \n",
    "    for c_idx, character in enumerate(table_id):\n",
    "        if character == '_' and not under_encountered:\n",
    "            start_idx = c_idx\n",
    "            under_encountered = True\n",
    "        elif character == '_' and under_encountered:\n",
    "            end_idx = c_idx\n",
    "    \n",
    "    in_between_str = table_id[start_idx:end_idx + 1]\n",
    "    \n",
    "    if in_between_str == '_succes_':\n",
    "        new_string = table_id[0:start_idx] + '_success' + table_id[end_idx:]\n",
    "        sheet_names[idx] = new_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: In the below cell, I use [:-1] to intentionally exclude the 'aggregate data' metric for the duplicates. I found that including this metric would ruin the heat map because the aggregate_value is naturally much higher and thus throws off the heat map's \"calibration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table_sheets = {}\n",
    "\n",
    "for name, sheet in zip(sheet_names, table_sheets):\n",
    "    sheet_cols = sheet.columns\n",
    "    sheet_cols = sheet_cols[2:]\n",
    "    new_df = pd.DataFrame(columns=sheet_cols)\n",
    "\n",
    "    for col in sheet_cols:\n",
    "        old_col = sheet[col]\n",
    "        new_col = pd.to_numeric(old_col, errors='coerce')\n",
    "        new_df[col] = new_col[:-1]\n",
    "\n",
    "    new_table_sheets[name] = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 12))\n",
    "sns.heatmap(new_table_sheets['condition_occurrence'], annot=True, annot_kws={\"size\": 10},\n",
    "            fmt='g', linewidths=.5, ax=ax, yticklabels=hpo_id_cols,\n",
    "            xticklabels=date_cols, cmap=\"YlGnBu\")\n",
    "# ax.set_title(sheet_names[0])\n",
    "ax.set_title(\"Condition Table Duplicates\", size=14)\n",
    "plt.savefig(\"condition_duplicates.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 12))\n",
    "sns.heatmap(new_table_sheets['drug_exposure'], annot=True, annot_kws={\"size\": 10},\n",
    "            fmt='g', linewidths=.5, ax=ax, yticklabels=hpo_id_cols,\n",
    "            xticklabels=date_cols, cmap=\"YlGnBu\")\n",
    "# ax.set_title(sheet_names[1])\n",
    "ax.set_title(\"Drug Table Duplicates\", size=14)\n",
    "plt.savefig(\"drug_duplicates.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 12))\n",
    "sns.heatmap(new_table_sheets['measurement'], annot=True, annot_kws={\"size\": 10},\n",
    "            fmt='g', linewidths=.5, ax=ax, yticklabels=hpo_id_cols,\n",
    "            xticklabels=date_cols, cmap=\"YlGnBu\")\n",
    "# ax.set_title(sheet_names[2])\n",
    "ax.set_title(\"Measurement Table Duplicates\", size=14)\n",
    "plt.savefig(\"measurement_duplicates.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 12))\n",
    "sns.heatmap(new_table_sheets['observation'], annot=True, annot_kws={\"size\": 10},\n",
    "            fmt='g', linewidths=.5, ax=ax, yticklabels=hpo_id_cols,\n",
    "            xticklabels=date_cols, cmap=\"YlGnBu\")\n",
    "# ax.set_title(sheet_names[3])\n",
    "ax.set_title(\"Observation Table Duplicates\", size=14)\n",
    "plt.savefig(\"observation_duplicates.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 12))\n",
    "sns.heatmap(new_table_sheets['procedure_occurrence'], annot=True, annot_kws={\"size\": 10},\n",
    "            fmt='g', linewidths=.5, ax=ax, yticklabels=hpo_id_cols,\n",
    "            xticklabels=date_cols, cmap=\"YlGnBu\")\n",
    "# ax.set_title(sheet_names[4])\n",
    "ax.set_title(\"Procedure Table Duplicates\", size=14)\n",
    "plt.savefig(\"procedure_duplicates.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 12))\n",
    "sns.heatmap(new_table_sheets['visit_occurrence'], annot=True, annot_kws={\"size\": 10},\n",
    "            fmt='g', linewidths=.5, ax=ax, yticklabels=hpo_id_cols,\n",
    "            xticklabels=date_cols, cmap=\"YlGnBu\")\n",
    "\n",
    "# ax.set_title(sheet_names[5])\n",
    "ax.set_title(\"Visit Table Duplicates\", size=14)\n",
    "\n",
    "plt.savefig(\"visit_duplicates.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a box-and-whisker plot for the different table types across all sites\n",
    "\n",
    "#### NOTE: This doesn't work super well. Not saving as an image. Might be helpful down the line so keeping it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = \"ticks\")\n",
    "f, ax = plt.subplots(figsize=(18, 12))\n",
    "\n",
    "date = 'september_16_2019'\n",
    "\n",
    "date_info = {}\n",
    "\n",
    "# need to generate the data from each table for a particular date\n",
    "for table_type in sheet_names:\n",
    "    date_info[table_type] = new_table_sheets[table_type][date].tolist()\n",
    "\n",
    "july_15_df = pd.DataFrame.from_dict(date_info)\n",
    "\n",
    "sns.boxplot(data=july_15_df, \n",
    "            whis = \"range\", palette=\"vlag\")\n",
    "\n",
    "sns.swarmplot(data=july_15_df,\n",
    "              size = 5, color=\".3\", linewidth=0)\n",
    "\n",
    "plt.ylabel(ylabel=\"Duplicate Numbers\", size=16)\n",
    "plt.xlabel(xlabel=\"\\nTable Type\", size=16)\n",
    "plt.title(\"Table Duplicate Numbers for {}\".format(date), size = 18)\n",
    "sns.despine(trim=True, left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's look at the metrics for particular sites with respect to duplicates; this will allow us to send them the same information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_name_list = ['aouw_mcri', 'aouw_mcw', 'aouw_uwh', 'chci', 'chs', 'cpmc_ceders', \n",
    "                  'cpmc_ucd', 'cpmc_uci', 'cpmc_ucsd', 'cpmc_ucsf', 'cpmc_usc', 'ecchc',\n",
    "                  'hrhc', 'ipmc_northshore', 'ipmc_nu', 'ipmc_rush', 'ipmc_uchicago',\n",
    "                  'ipmc_uic', 'jhchc', 'nec_bmc', 'nec_phs', 'nyc_cornell', 'nyc_cu',\n",
    "                  'nyc_hh', 'pitt', 'saou_uab', 'saou_ummc', 'seec_emory', 'seec_miami',\n",
    "                  'seec_morehouse', 'seec_ufl', 'syhc', 'tach_hfhs', 'trans_am_baylor',\n",
    "                  'trans_am_essentia', 'trans_am_spectrum', 'uamc_banner', 'aggregate_info']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell for the CDR; trans_am_essentia and saou_ummc taken out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "site_name_list = ['aouw_mcri', 'aouw_mcw', 'aouw_uwh', 'chci', 'chs', 'cpmc_ceders', \n",
    "                  'cpmc_ucd', 'cpmc_uci', 'cpmc_ucsd', 'cpmc_ucsf', 'cpmc_usc', 'ecchc',\n",
    "                  'hrhc', 'ipmc_northshore', 'ipmc_nu', 'ipmc_rush', 'ipmc_uchicago',\n",
    "                  'ipmc_uic', 'jhchc', 'nec_bmc', 'nec_phs', 'nyc_cornell', 'nyc_cu',\n",
    "                  'nyc_hh', 'pitt', 'saou_uab', 'seec_emory', 'seec_miami',\n",
    "                  'seec_morehouse', 'seec_ufl', 'syhc', 'tach_hfhs', 'trans_am_baylor',\n",
    "                  'trans_am_spectrum', 'uamc_banner', 'aggregate_info',\n",
    "                  'poorly_defined_rows_total', 'total_rows']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_interest = 'aggregate_info'\n",
    "\n",
    "if name_of_interest not in site_name_list:\n",
    "    raise ValueError(\"Name not found in the list of HPO site names.\")    \n",
    "\n",
    "for idx, site in enumerate(site_name_list):\n",
    "    if site == name_of_interest:\n",
    "        idx_of_interest = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn1_hpo_sheets = 'duplicates_hpo_sheets_data_analytics.xlsx'\n",
    "file_names_hpo_sheets = [fn1_hpo_sheets]\n",
    "\n",
    "s1, s2 = site_name_list[0], site_name_list[1]\n",
    "s3, s4 = site_name_list[2], site_name_list[3]\n",
    "s5, s6 = site_name_list[4], site_name_list[5]\n",
    "s7, s8 = site_name_list[6], site_name_list[7]\n",
    "s9, s10 = site_name_list[8], site_name_list[9]\n",
    "s11, s12 = site_name_list[10], site_name_list[11]\n",
    "s13, s14 = site_name_list[12], site_name_list[13]\n",
    "s15, s16 = site_name_list[14], site_name_list[15]\n",
    "s17, s18 = site_name_list[16], site_name_list[17]\n",
    "s19, s20 = site_name_list[18], site_name_list[19]\n",
    "s21, s22 = site_name_list[20], site_name_list[21]\n",
    "s23, s24 = site_name_list[22], site_name_list[23]\n",
    "s25, s26 = site_name_list[24], site_name_list[25]\n",
    "s27, s28 = site_name_list[26], site_name_list[27]\n",
    "s29, s30 = site_name_list[28], site_name_list[29]\n",
    "s31, s32 = site_name_list[30], site_name_list[31]\n",
    "s33, s34 = site_name_list[32], site_name_list[33]\n",
    "s35, s36 = site_name_list[34], site_name_list[35]\n",
    "s37, s38 = site_name_list[36], site_name_list[37]\n",
    "\n",
    "hpo_sheet_names = [\n",
    "    s1, s2, s3, s4, s5, s6, s7, s8, s9, s10, s11, s12, s13, s14, \n",
    "    s15, s16, s17, s18, s19, s20, s21, s22, s23, s24, s25, s26,\n",
    "    s27, s28, s29, s30, s31, s32, s33, s34, s35, s36, s37, s38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_sheets = []\n",
    "\n",
    "for file in file_names_hpo_sheets:\n",
    "    for sheet in hpo_sheet_names:\n",
    "        s = pd.read_excel(file, sheet)\n",
    "        hpo_sheets.append(s)\n",
    "\n",
    "table_id_cols = list(hpo_sheets[0]['table_type'])\n",
    "date_cols = hpo_sheets[0].columns[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hpo_sheets = []\n",
    "\n",
    "for sheet in hpo_sheets:\n",
    "    sheet_cols = sheet.columns\n",
    "    sheet_cols = sheet_cols[2:]  # first two do not have data\n",
    "    new_df = pd.DataFrame(columns=sheet_cols)\n",
    "\n",
    "    for col in sheet_cols:\n",
    "        old_col = sheet[col]\n",
    "        new_col = pd.to_numeric(old_col, errors='coerce')\n",
    "        new_df[col] = new_col\n",
    "\n",
    "    new_hpo_sheets.append(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing for one particular site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "sns.heatmap(new_hpo_sheets[idx_of_interest], annot=True, annot_kws={\"size\": 14},\n",
    "            fmt='g', linewidths=.5, ax=ax, yticklabels=table_id_cols,\n",
    "            xticklabels=date_cols, cmap=\"YlGnBu\")\n",
    "\n",
    "ax.set_title(\"Number of Duplicates for {}\".format(name_of_interest), size=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "img_name = name_of_interest + \"_duplicate_number.jpg\"\n",
    "\n",
    "plt.savefig(img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = new_hpo_sheets[0].columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: This is more experimental than anything else; Could be improved upon in the future. This particular graphic is also only quasi-informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tables = len(new_hpo_sheets[0]) - 1  # do not include aggregate\n",
    "\n",
    "# Angle for the circle; divide by number of variables\n",
    "angles = [(angle / num_tables) * (2 * pi) for angle in range(num_tables)]\n",
    "angles += angles[:1] # back to the start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = 0\n",
    "\n",
    "site = new_hpo_sheets[idx_of_interest]\n",
    "site_name = site_name_list[idx_of_interest]\n",
    "\n",
    "for date in dates:\n",
    "    date_vals = site[date].values\n",
    "    date_vals = date_vals.flatten().tolist()[:-1]  # cut off aggregate\n",
    "    \n",
    "    for value in date_vals:\n",
    "        if value > max_val:\n",
    "            max_val = value\n",
    "\n",
    "y_ticks = [max_val / 5, max_val * 2 / 5, max_val * 3 / 5, max_val * 4 / 5, max_val]\n",
    "y_ticks_str = []\n",
    "\n",
    "for val in y_ticks:\n",
    "    string = str(val)\n",
    "    y_ticks_str.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "ax = plt.subplot(111, polar = True)  # initialize\n",
    "\n",
    "ax.set_theta_offset(pi / 2)  # flip to top\n",
    "ax.set_theta_direction(-1)\n",
    "\n",
    "plt.xticks(angles[:-1],table_id_cols)\n",
    "\n",
    "# Draw ylabels\n",
    "ax.set_rlabel_position(0)\n",
    "plt.yticks(y_ticks, y_ticks_str, color=\"grey\", size=8)\n",
    "plt.ylim(0, max_val)\n",
    "\n",
    "\n",
    "for date_idx in range(len(dates)):\n",
    "    date_vals = site[dates[date_idx]].values\n",
    "    date = date_vals.flatten().tolist()[:-1]  # cut off aggregate\n",
    "    date += date[:1]  # round out the graph\n",
    "    \n",
    "    ax.plot(angles, date, linewidth=1, linestyle='solid', label=dates[date_idx])\n",
    "    ax.fill(angles, date, alpha=0.1)\n",
    "\n",
    "plt.title(\"Duplicate Number: {}\".format(name_of_interest), size=15, y = 1.1)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = new_hpo_sheets[idx_of_interest].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want a line chart over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hpo_sheets[idx_of_interest]\n",
    "\n",
    "times=new_hpo_sheets[idx_of_interest].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_rates = {}\n",
    "\n",
    "for table_num, table_type in enumerate(table_id_cols):\n",
    "    table_metrics_over_time = new_hpo_sheets[idx_of_interest].iloc[table_num]\n",
    "    success_rates[table_type] = table_metrics_over_time.values.tolist()\n",
    "\n",
    "date_idxs = []\n",
    "for x in range(len(dates)):\n",
    "    date_idxs.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table, values_over_time in success_rates.items():\n",
    "    sample_list = [x for x in success_rates[table] if str(x) != 'nan']\n",
    "    if len(sample_list) > 1:\n",
    "        plt.plot(date_idxs, success_rates[table], '--', label=table)\n",
    "    \n",
    "for table, values_over_time in success_rates.items():\n",
    "    non_nan_idx = 0\n",
    "    new_lst = []\n",
    "    \n",
    "    for idx, x in enumerate(success_rates[table]):\n",
    "        if str(x) != 'nan':\n",
    "            new_lst.append(x)\n",
    "            non_nan_idx = idx\n",
    "    \n",
    "    if len(new_lst) == 1:\n",
    "        plt.plot(date_idxs[non_nan_idx], new_lst, 'o', label=table)\n",
    "\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\n",
    "plt.title(\"Number of Duplicates for {}\".format(site_name_list[idx_of_interest]))\n",
    "plt.ylabel(\"Duplicate Number\")\n",
    "plt.xlabel(\"\")\n",
    "plt.xticks(date_idxs, times, rotation = 'vertical')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "lgd = ax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5,-0.1))\n",
    "\n",
    "img_name = name_of_interest + \"_duplicates_line_graph.jpg\"\n",
    "plt.savefig(img_name, bbox_extraartist=(lgd,), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
